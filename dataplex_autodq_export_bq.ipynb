{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b7c8f-6cc0-4181-ae23-02f4d514ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-dataplex\n",
    "!pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdb0db-804c-43b2-a804-912bf2332b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dataplex_autodq_export_bq.py\n",
    "'''\n",
    "This script exports Data Quality (DQ) results (jobs and rules) from a\n",
    " Dataplex AutoDQ Scan to BigQuery. It creates 2 tables in BigQuery:\n",
    "  `my-dataset.my_table_per_job` and `my-dataset.my_table_per_rule`.\n",
    "\n",
    "Arguments:\n",
    "--datascan_name: The full project path of the Datascan.\n",
    "--dataset: The full Dataset ID: project.dataset.\n",
    "--table: The base table name to create for DQ results. This will be created as `table_per_job`.\n",
    "--dataset_location: The location of your dataset, e.g. us-central1.\n",
    "\n",
    "Usage:\n",
    "python dataplex_dq_export_bq.py \\\n",
    "--datascan_name=\"my-datascan\" \\\n",
    "--dataset=\"my-dataset\" \\\n",
    "--table=\"my-table\" \\\n",
    "--dataset_location=\"us-central1\"\n",
    "'''\n",
    "\n",
    "from google.cloud import dataplex_v1\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import argparse\n",
    "import time\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--datascan_name', '-dsn', help='Full project path of the Datascan')\n",
    "parser.add_argument('--dataset', '-dst', help='Full Dataset ID: project.dataset')\n",
    "parser.add_argument('--table', '-tbl', help='Base table name to create for DQ results. This will be created as `table_per_job`')\n",
    "parser.add_argument('--dataset_location', '-loc', help='Location of your dataset, e.g. us-central1')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "def export_data(parent):\n",
    "    # replace Service Account keys with your .json credentials\n",
    "    key_path = \"/<example>/<sample>.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "    )\n",
    "    bq_client = bigquery.Client(\n",
    "        credentials=credentials, project=credentials.project_id,)\n",
    "\n",
    "    dataset = bigquery.Dataset(\n",
    "        dataset_ref=args.dataset\n",
    "    )\n",
    "    dataset.location = args.dataset_location\n",
    "    bq_client.create_dataset(dataset=dataset, exists_ok=True)\n",
    "\n",
    "    schema_per_job = [\n",
    "        bigquery.SchemaField(name=\"datascan_id\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"table_id\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"job_id\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"records_scannned\", field_type=\"INT64\"),\n",
    "        bigquery.SchemaField(name=\"passed\", field_type=\"BOOLEAN\"),\n",
    "        bigquery.SchemaField(name=\"total_rules\", field_type=\"INT64\"),\n",
    "        bigquery.SchemaField(name=\"passing_rules\", field_type=\"INT64\"),\n",
    "        bigquery.SchemaField(name=\"job_start_time\", field_type=\"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(name=\"job_end_time\", field_type=\"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(name=\"scanned_data\", field_type=\"JSON\"),\n",
    "        bigquery.SchemaField(name=\"rule_results\", field_type=\"JSON\"),\n",
    "    ]\n",
    "\n",
    "    schema_per_rule = [\n",
    "        bigquery.SchemaField(name=\"job_name\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"datascan\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"job\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"rule_config\", field_type=\"JSON\"),\n",
    "        bigquery.SchemaField(name=\"dimension\", field_type=\"STRING\"),\n",
    "        bigquery.SchemaField(name=\"passed\", field_type=\"BOOLEAN\"),\n",
    "        bigquery.SchemaField(name=\"pass_ratio\", field_type=\"FLOAT\"),\n",
    "        bigquery.SchemaField(name=\"debug_query\", field_type=\"STRING\"),\n",
    "    ]\n",
    "\n",
    "    table_per_job = bigquery.Table(\n",
    "        table_ref=args.dataset + \".\" + args.table + \"_per_job\",\n",
    "        schema=schema_per_job,\n",
    "    )\n",
    "    bq_client.create_table(table=table_per_job, exists_ok=True)\n",
    "\n",
    "    table_per_rule = bigquery.Table(\n",
    "        table_ref=args.dataset + \".\" + args.table + \"_per_rule\",\n",
    "        schema=schema_per_rule,\n",
    "    )\n",
    "    bq_client.create_table(table=table_per_rule, exists_ok=True)\n",
    "\n",
    "    # Get Jobs Data\n",
    "    print(\"Getting DQ Jobs Data\")\n",
    "    client = dataplex_v1.DataScanServiceClient(credentials=credentials,)\n",
    "\n",
    "    request_scan = dataplex_v1.GetDataScanRequest(name=args.datascan_name)\n",
    "    response_scan = client.get_data_scan(request=request_scan)\n",
    "\n",
    "    request = dataplex_v1.ListDataScanJobsRequest(\n",
    "        parent=args.datascan_name, page_size=10)\n",
    "\n",
    "    page_result = client.list_data_scan_jobs(request=request)\n",
    "    counter = 0\n",
    "    job_names = []\n",
    "    for response in page_result:\n",
    "        counter += 1\n",
    "        job_names.append(response.name)\n",
    "        # Limit to only 5 jobs by uncommenting below\n",
    "        # if counter == 5:\n",
    "        #   break\n",
    "        if counter % 60 == 0:\n",
    "            time.sleep(30)\n",
    "            # break\n",
    "            print(\"Waiting 30 seconds before fetching more jobs\")\n",
    "\n",
    "    print('Jobs scanned: ' + str(counter))\n",
    "    print(*job_names, sep=\"\\n\")\n",
    "\n",
    "    # Write Jobs data to bigquery\n",
    "    print(\"Writing Jobs data to bigquery:\")\n",
    "    for job_name in job_names:\n",
    "        print(job_name)\n",
    "        job_request = dataplex_v1.GetDataScanJobRequest(\n",
    "            name=job_name,\n",
    "            view=\"FULL\",\n",
    "        )\n",
    "        job_result = client.get_data_scan_job(request=job_request)\n",
    "        # Skips jobs if not in succeeded state\n",
    "        if job_result.state != 4:\n",
    "            continue\n",
    "\n",
    "        split_job = job_result.name.split(\"/\")\n",
    "        passing_rules = 0\n",
    "        failing_rules = 0\n",
    "        for rule in job_result.data_quality_result.rules:\n",
    "            if rule.passed is True:\n",
    "                passing_rules += 1\n",
    "            elif rule.passed is False:\n",
    "                failing_rules += 1\n",
    "        print(' -->Passing rules = ' + str(passing_rules))\n",
    "        print(' -->Failing rules = ' + str(failing_rules))\n",
    "\n",
    "        bq_client.insert_rows(\n",
    "            table=args.dataset + \".\" + args.table + \"_per_job\",\n",
    "            rows=[(\n",
    "                split_job[1] + \"/\" + split_job[5],\n",
    "                response_scan.data.entity,\n",
    "                job_result.uid,\n",
    "                job_result.data_quality_result.row_count,\n",
    "                job_result.data_quality_result.passed,\n",
    "                len(job_result.data_quality_result.rules),\n",
    "                passing_rules,\n",
    "                job_result.start_time,\n",
    "                job_result.end_time,\n",
    "                MessageToJson(job_result.data_quality_result.scanned_data._pb),\n",
    "                MessageToJson(job_result.data_quality_result._pb),\n",
    "            )],\n",
    "            selected_fields=schema_per_job,\n",
    "        )\n",
    "\n",
    "        for rule_result in job_result.data_quality_result.rules:\n",
    "            bq_client.insert_rows(\n",
    "                table=args.dataset + \".\" + args.table + \"_per_rule\",\n",
    "                rows=[(\n",
    "                    job_result.name,\n",
    "                    split_job[5],\n",
    "                    split_job[7],\n",
    "                    MessageToJson(rule_result.rule._pb),\n",
    "                    rule_result.rule.dimension,\n",
    "                    rule_result.passed,\n",
    "                    rule_result.pass_ratio,\n",
    "                    rule_result.failing_rows_query\n",
    "                )],\n",
    "                selected_fields=schema_per_rule,\n",
    "            )\n",
    "\n",
    "\n",
    "export_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38266b5-d7db-4fe0-a4ca-fc8dced6513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your project, datascan, dataset, etc. before running:\n",
    "\n",
    "%run dataplex_autodq_export_bq.py \\\n",
    "--datascan projects/<project_name>/locations/<region>/dataScans/<data-scan-name> \\\n",
    "--dataset <project.dataset> \\\n",
    "--table <base_table_name> \\\n",
    "--dataset_location <location>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
